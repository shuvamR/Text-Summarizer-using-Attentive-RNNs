<h1>Text-Summarizer-using-Attentive-RNNs</h1>
In this project, we have implemented a Text Summarizer using Attentive RNNs for a dataset and also we have tested on our custom input too. So basically, Text Summarizer or Text abstraction is a way to reduce long text into smaller fragments. The motive is to create a regular and articulate summary that consists of most of the principal factors mentioned in the document. Text summarization is a not unusual difficulty in neural networks and natural language processing & additionally, machine learning. As NN seq2seq models have potential to provide ingenious methods to extract out the text summary just by rearranging and selecting paragraphs from given text. But the working models holds some limitations and flaws i.e. they may be prone to reproduce real information inaccurately, and that they have a tendency to copy themselves . In this project we've used well known Long Short-Term Memory(LSTM) sequence to sequence attentional version. This approach makes use of a nearby interest version for producing every phase of the precise condition at the enter sentence. While the version is structurally simple, it may effortlessly learn cease-to-cease and scales to a massive quantity of schooling data. By using Tensorflow neural network library and with the help of Amazon Fine Food Reviews as a dataset we have implemented the same. We have also tested the same for our custom input. At the end we examine the reconstructed paragraph the use of standard metrics, with the aid of using, we are able to show that neural network models can encode texts in a way that keeps discourse, syntactic, and semantic coherence.

Conclusion: We have implemented Attentive RNN Model for Text Summarizer. We have used learning model which is untangled version for encoder and decoder configuration . Training of the model is done on the dataset i.e. “Amazon Fine Food Review” which further initiate summary for the reviews using the sentences and lines from every review. We also noticed some limitations as the approach we used kind of outdated which can be worked out and improved in the further works. The very 1st limitation which we noticed that, it sometimes replicates same words again and again in summary, along with that when input text size is large its takes too much time. Other Issue, which is mainly related to larger input sometimes during implementation the working model misinterprets the literal meanings which ultimately results in wrong results or undesired results. Also it had issue implementing on Tensorflow of version 2.0 or above although it can optimized using other alternatives.

Dataset Used: [Kaggle](https://www.kaggle.com/snap/amazon-fine-food-reviews)

Pre-Trained Encoder: [Conceptnet Numberbatch's](https://github.com/commonsense/conceptnet-numberbatch)

Reference: [GitHub page.](https://github.com/tensorflow/models/tree/master/textsum)

